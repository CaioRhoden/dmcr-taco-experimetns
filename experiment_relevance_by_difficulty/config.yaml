

inference_config:
  instruction: "You are a coding generation tool that will solve a problem using Python"
  saving_path: "generations"
  model_path: "../models/llm/Llama-3.2-3B-Instruct"
  num_returns: 20
  num_generations: 20
  log_datetime: False

model_config:
  temperature: 0.7
  top_p: 0.95
  max_length: 2048

parse_config:
  saving_path: "parsed_generations"

results_config:
  saving_path: "results"
  log_only_total: True
  