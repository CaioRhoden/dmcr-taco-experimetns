{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import uuid\n",
    "import torch\n",
    "import numpy as np\n",
    "seed = 42\n",
    "import random\n",
    "# NumPy\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "# PyTorch\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio.rhoden/miniconda3/envs/taco/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "pl.set_random_seed(seed)\n",
    "\n",
    "import json\n",
    "from datasets import load_from_disk\n",
    "import yaml\n",
    "import wandb\n",
    "\n",
    "## Repo funtions\n",
    "from taco_utils.evaluators.TACOEvaluator import TACOEvaluator\n",
    "from taco_utils import run_inference, parse_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ../data/TACO/train.hf not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m train_solutions = pl.read_ipc(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/train_solutions.feather\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m train_tests = pl.read_ipc(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/train_evaluation_tests.feather\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m train_dict = \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/TACO/train.hf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/taco/lib/python3.11/site-packages/datasets/load.py:2139\u001b[39m, in \u001b[36mload_from_disk\u001b[39m\u001b[34m(dataset_path, keep_in_memory, storage_options)\u001b[39m\n\u001b[32m   2137\u001b[39m fs, *_ = url_to_fs(dataset_path, **(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[32m   2138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs.exists(dataset_path):\n\u001b[32m-> \u001b[39m\u001b[32m2139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fs.isfile(posixpath.join(dataset_path, config.DATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs.isfile(\n\u001b[32m   2141\u001b[39m     posixpath.join(dataset_path, config.DATASET_STATE_JSON_FILENAME)\n\u001b[32m   2142\u001b[39m ):\n\u001b[32m   2143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Directory ../data/TACO/train.hf not found"
     ]
    }
   ],
   "source": [
    "PATH  = \"../data/TACO/processed\"\n",
    "train = pl.read_ipc(f\"{PATH}/train.feather\")\n",
    "test = pl.read_ipc(f\"{PATH}/test.feather\")\n",
    "train_solutions = pl.read_ipc(f\"{PATH}/train_solutions.feather\")\n",
    "train_tests = pl.read_ipc(f\"{PATH}/train_evaluation_tests.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluation_tests = pl.read_ipc(f\"{PATH}/test_evaluation_tests.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (287, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>test_id</th><th>input</th><th>output</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>102</td><td>0</td><td>&quot;6 2 2\n",
       "1 1 2 2 1 1\n",
       "&quot;</td><td>&quot;6\n",
       "&quot;</td></tr><tr><td>102</td><td>1</td><td>&quot;1 1 1\n",
       "1\n",
       "&quot;</td><td>&quot;0\n",
       "&quot;</td></tr><tr><td>102</td><td>2</td><td>&quot;10 2 1\n",
       "2 1 2 2 1 2 2 1 1 2\n",
       "&quot;</td><td>&quot;5\n",
       "&quot;</td></tr><tr><td>102</td><td>3</td><td>&quot;50 2 1\n",
       "1 1 2 2 1 2 1 1 2 2 1 2…</td><td>&quot;15\n",
       "&quot;</td></tr><tr><td>102</td><td>4</td><td>&quot;75 5 5\n",
       "1 1 5 5 3 5 2 3 3 2 2 1…</td><td>&quot;6\n",
       "&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>102</td><td>282</td><td>&quot;6 31 7\n",
       "10 -1 7 14 6 3\n",
       "&quot;</td><td>&quot;0\n",
       "&quot;</td></tr><tr><td>102</td><td>283</td><td>&quot;100 100 100\n",
       "1 2 3 4 5 6 7 8 9 …</td><td>&quot;0\n",
       "&quot;</td></tr><tr><td>102</td><td>284</td><td>&quot;100 50 22\n",
       "15 2 25 15 48 43 46 …</td><td>&quot;2\n",
       "&quot;</td></tr><tr><td>102</td><td>285</td><td>&quot;1 1 1\n",
       "1\n",
       "&quot;</td><td>&quot;0&quot;</td></tr><tr><td>102</td><td>286</td><td>&quot;6 2 2\n",
       "1 1 2 2 1 1\n",
       "&quot;</td><td>&quot;6&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (287, 4)\n",
       "┌─────┬─────────┬──────────────────────────┬────────┐\n",
       "│ id  ┆ test_id ┆ input                    ┆ output │\n",
       "│ --- ┆ ---     ┆ ---                      ┆ ---    │\n",
       "│ i64 ┆ i64     ┆ str                      ┆ str    │\n",
       "╞═════╪═════════╪══════════════════════════╪════════╡\n",
       "│ 102 ┆ 0       ┆ 6 2 2                    ┆ 6      │\n",
       "│     ┆         ┆ 1 1 2 2 1 1              ┆        │\n",
       "│     ┆         ┆                          ┆        │\n",
       "│ 102 ┆ 1       ┆ 1 1 1                    ┆ 0      │\n",
       "│     ┆         ┆ 1                        ┆        │\n",
       "│     ┆         ┆                          ┆        │\n",
       "│ 102 ┆ 2       ┆ 10 2 1                   ┆ 5      │\n",
       "│     ┆         ┆ 2 1 2 2 1 2 2 1 1 2      ┆        │\n",
       "│     ┆         ┆                          ┆        │\n",
       "│ 102 ┆ 3       ┆ 50 2 1                   ┆ 15     │\n",
       "│     ┆         ┆ 1 1 2 2 1 2 1 1 2 2 1 2… ┆        │\n",
       "│ 102 ┆ 4       ┆ 75 5 5                   ┆ 6      │\n",
       "│     ┆         ┆ 1 1 5 5 3 5 2 3 3 2 2 1… ┆        │\n",
       "│ …   ┆ …       ┆ …                        ┆ …      │\n",
       "│ 102 ┆ 282     ┆ 6 31 7                   ┆ 0      │\n",
       "│     ┆         ┆ 10 -1 7 14 6 3           ┆        │\n",
       "│     ┆         ┆                          ┆        │\n",
       "│ 102 ┆ 283     ┆ 100 100 100              ┆ 0      │\n",
       "│     ┆         ┆ 1 2 3 4 5 6 7 8 9 …      ┆        │\n",
       "│ 102 ┆ 284     ┆ 100 50 22                ┆ 2      │\n",
       "│     ┆         ┆ 15 2 25 15 48 43 46 …    ┆        │\n",
       "│ 102 ┆ 285     ┆ 1 1 1                    ┆ 0      │\n",
       "│     ┆         ┆ 1                        ┆        │\n",
       "│     ┆         ┆                          ┆        │\n",
       "│ 102 ┆ 286     ┆ 6 2 2                    ┆ 6      │\n",
       "│     ┆         ┆ 1 1 2 2 1 1              ┆        │\n",
       "│     ┆         ┆                          ┆        │\n",
       "└─────┴─────────┴──────────────────────────┴────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluation_tests.filter(pl.col(\"id\") == 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Ad-hoc', array([190], dtype=uint32)],\n",
       "       ['Amortized analysis', array([102], dtype=uint32)],\n",
       "       ['Bit manipulation', array([165], dtype=uint32)],\n",
       "       ['Combinatorics', array([273], dtype=uint32)],\n",
       "       ['Complete search', array([780], dtype=uint32)],\n",
       "       ['Constructive algorithms', array([143], dtype=uint32)],\n",
       "       ['Data structures', array([291], dtype=uint32)],\n",
       "       ['Dynamic programming', array([283], dtype=uint32)],\n",
       "       ['Fundamentals', array([236], dtype=uint32)],\n",
       "       ['Game theory', array([111], dtype=uint32)],\n",
       "       ['Geometry', array([154], dtype=uint32)],\n",
       "       ['Graph algorithms', array([284], dtype=uint32)],\n",
       "       ['Graph traversal', array([247], dtype=uint32)],\n",
       "       ['Greedy algorithms', array([321], dtype=uint32)],\n",
       "       ['Implementation', array([149], dtype=uint32)],\n",
       "       ['Mathematics', array([168], dtype=uint32)],\n",
       "       ['Matrices', array([199], dtype=uint32)],\n",
       "       ['Number theory', array([222], dtype=uint32)],\n",
       "       ['Probability', array([103], dtype=uint32)],\n",
       "       ['Shortest paths', array([154], dtype=uint32)],\n",
       "       ['Sorting', array([143], dtype=uint32)],\n",
       "       ['Spanning trees', array([284], dtype=uint32)],\n",
       "       ['String algorithms', array([329], dtype=uint32)],\n",
       "       ['Tree algorithms', array([195], dtype=uint32)]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_filter = (\n",
    "        test\n",
    "        .filter(pl.col(\"difficulty\") == \"MEDIUM\")\n",
    "        .group_by(\"tags\")\n",
    "        .agg(pl.col(\"id\").count().alias(\"count\"))\n",
    "        .filter(pl.col(\"count\") >= 5)\n",
    "        .select(\"tags\")\n",
    "\n",
    "    )\n",
    "\n",
    "df = (\n",
    "            test\n",
    "            .filter(pl.col(\"difficulty\") == \"MEDIUM\")\n",
    "            .join(_filter, on=\"tags\", how=\"inner\")\n",
    "            .group_by(\"tags\")\n",
    "            .agg(pl.col(\"id\").sample(n=1, shuffle=True, with_replacement=False,seed=42))\n",
    "            .sort(\"tags\")\n",
    "            )\n",
    "\n",
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ad-hoc': {190: [4527, 8409, 19779, 15180]},\n",
       " 'Amortized analysis': {102: [7154, 6976, 17413, 12907]},\n",
       " 'Bit manipulation': {165: [4415, 18815, 13743, 4351]},\n",
       " 'Combinatorics': {273: [7332, 18824, 14330, 3468]},\n",
       " 'Complete search': {780: [4056, 6938, 6683, 18526]},\n",
       " 'Constructive algorithms': {143: [4527, 9348, 9297, 19493]},\n",
       " 'Data structures': {291: [4390, 7646, 7399, 18540]},\n",
       " 'Dynamic programming': {283: [3839, 7493, 7071, 18926]},\n",
       " 'Fundamentals': {236: [21214, 6617, 12329, 11405]},\n",
       " 'Game theory': {111: [5085, 23895, 7220, 17412]},\n",
       " 'Geometry': {154: [3281, 24864, 6474, 16045]},\n",
       " 'Graph algorithms': {284: [4795, 8243, 8088, 711]},\n",
       " 'Graph traversal': {247: [3348, 7179, 3321, 8018]},\n",
       " 'Greedy algorithms': {321: [4079, 8628, 8324, 19424]},\n",
       " 'Implementation': {149: [3954, 7893, 7643, 19566]},\n",
       " 'Mathematics': {168: [3783, 7143, 18669, 7942]},\n",
       " 'Matrices': {199: [24682, 4528, 6774, 17882]},\n",
       " 'Number theory': {222: [3457, 6213, 5888, 17926]},\n",
       " 'Probability': {103: [7533, 11762, 3131, 10707]},\n",
       " 'Shortest paths': {154: [23845, 5515, 8634, 9037]},\n",
       " 'Sorting': {143: [7978, 7785, 19227, 4084]},\n",
       " 'Spanning trees': {284: [24190, 9037, 18958, 14284]},\n",
       " 'String algorithms': {329: [8513, 8226, 14760, 8984]},\n",
       " 'Tree algorithms': {195: [3407, 17665, 12314, 3269]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = {\n",
    "        \"input_ids\": {},\n",
    "        \"context_ids\": {}\n",
    "    }\n",
    "for _tag in df.to_numpy():\n",
    "    input_data[\"input_ids\"][_tag[0]] = _tag[1].tolist()\n",
    "\n",
    "\n",
    "\n",
    "context = {}\n",
    "for _key in  input_data[\"input_ids\"].keys():\n",
    "        input_data[\"context_ids\"][_key] = {}\n",
    "        for _id in  input_data[\"input_ids\"][_key]:\n",
    "            _df = (\n",
    "                train\n",
    "                .filter(pl.col(\"difficulty\") == \"MEDIUM\")\n",
    "                .filter(pl.col(\"tags\") == _key)\n",
    "                .sample(n=4, shuffle=True, with_replacement=False, seed=42)\n",
    "            )\n",
    "\n",
    "            input_data[\"context_ids\"][_key][_id] = _df.select(pl.col(\"id\")).to_numpy().squeeze(1).tolist()\n",
    "\n",
    "\n",
    "\n",
    "input_data[\"context_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inference_configs': {'instruction': 'You are a coding generation tool that will solve a problem using Python',\n",
       "  'saving_path': 'generations',\n",
       "  'model_path': '../models/llms/Llama-3.2-3B-Instruct',\n",
       "  'num_returns': 20,\n",
       "  'num_generations': 200,\n",
       "  'log_datetime': False,\n",
       "  'quantization': True,\n",
       "  'start_idx': 0,\n",
       "  'end_idx': 120},\n",
       " 'model_configs': {'temperature': 0.7, 'top_p': 0.95, 'max_length': 2048},\n",
       " 'parse_configs': {'saving_path': 'parsed'},\n",
       " 'results_configs': {'saving_path': 'results', 'log_only_total': True}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_problem = train.filter(pl.col(\"id\") == 2545)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = selected_problem.select(\"input\").to_struct().to_pandas().iloc[0][\"input\"]\n",
    "prompt = f\"Please write a Python program \\nQUESTION: \\n{prompt_input} \\n ANSWER: \\n.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mc214129\u001b[0m (\u001b[33mc214129-unicamp\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path logs/wandb/ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20250319_172753-2545</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/c214129-unicamp/dmcr-taco-experiment-difficulty-relevance/runs/2545' target=\"_blank\">2545</a></strong> to <a href='https://wandb.ai/c214129-unicamp/dmcr-taco-experiment-difficulty-relevance' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/c214129-unicamp/dmcr-taco-experiment-difficulty-relevance' target=\"_blank\">https://wandb.ai/c214129-unicamp/dmcr-taco-experiment-difficulty-relevance</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/c214129-unicamp/dmcr-taco-experiment-difficulty-relevance/runs/2545' target=\"_blank\">https://wandb.ai/c214129-unicamp/dmcr-taco-experiment-difficulty-relevance/runs/2545</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it]\n",
      "/home/caio.rhoden/miniconda3/envs/taco/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, -3, -3, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, -1, -1], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, True, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, True, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [-3, -3, -3, -3, -3, -3, -3, -3, -3, -3], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, True, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False]]\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 3, 0]\n",
      "200 10\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2545</strong> at: <a href='https://wandb.ai/c214129-unicamp/dmcr-taco-experiment-difficulty-relevance/runs/2545' target=\"_blank\">https://wandb.ai/c214129-unicamp/dmcr-taco-experiment-difficulty-relevance/runs/2545</a><br> View project at: <a href='https://wandb.ai/c214129-unicamp/dmcr-taco-experiment-difficulty-relevance' target=\"_blank\">https://wandb.ai/c214129-unicamp/dmcr-taco-experiment-difficulty-relevance</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/tmp/wandb/run-20250319_172753-2545/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project = \"dmcr-taco-experiment-difficulty-relevance\", \n",
    "    dir = \"logs\",\n",
    "    id = f\"2545\", \n",
    "    name = f\"2545\",\n",
    "    config = config,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# run_inference(\n",
    "#     prompt = prompt_input,\n",
    "#     instruction = config[\"inference_configs\"][\"instruction\"],\n",
    "#     saving_path = f\"{config['inference_configs']['saving_path']}/no_context.json\",\n",
    "#     model_path = config[\"inference_configs\"][\"model_path\"],\n",
    "#     model_configs = config[\"model_configs\"],\n",
    "#     num_returns = config[\"inference_configs\"][\"num_returns\"],\n",
    "#     num_generations = config[\"inference_configs\"][\"num_generations\"],\n",
    "#     log_datetime = config[\"inference_configs\"][\"log_datetime\"],\n",
    "#     quantization = config[\"inference_configs\"][\"quantization\"]\n",
    "    \n",
    "# )\n",
    "\n",
    "# parse_generations(\n",
    "#     generations_path=f\"{config['inference_configs']['saving_path']}/no_context.json\",\n",
    "#     id = 2545,\n",
    "#     saving_path = f\"{config['parse_configs']['saving_path']}/no_context_parsed.json\"\n",
    "# )\n",
    "\n",
    "evaluator = TACOEvaluator(\n",
    "    generation_file = f\"{config['parse_configs']['saving_path']}/no_context_parsed.json\",\n",
    "    taco = [train_dict[2545]],\n",
    "    k_pass = [1, 10, 100],\n",
    "    k_pass_path = f\"{config['results_configs']['saving_path']}/no_context_1_pass.json\",\n",
    "    normalized_sum_path = f\"{config['results_configs']['saving_path']}/no_context_normalized_sum.json\"\n",
    ")\n",
    "\n",
    "input_id = str(uuid.uuid4())\n",
    "with open(f\"logs/{input_id}.txt\", \"w\") as f:\n",
    "    f.write(prompt_input, \"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluator.evaluate()\n",
    "\n",
    "wandb.log({\n",
    "    \"pass@1\": evaluator.extract_pass_1(),\n",
    "    \"normalized_sum\": evaluator.extracted_normalized_sum(),\n",
    "})\n",
    "wandb.log({\"prompt\": wandb.File(f\"logs/{input_id}.txt\")})\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "# parse_generation(json.load(open(\"no_context.json\")), 2545 , \"no_context_parsed.json\")\n",
    "# compute_1_pass_by_test(\"no_context_parsed.json\", [train_dict[2545]], file=\"no_context_1_pass.json\")\n",
    "# results = json.load(open(\"no_context_1_pass.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"It's the rainy season again, and the city experiences frequent showers throughout the day.\\n\\nThe weather report says that there is a P probability of rainfalls today. Raj has to step out for a meeting at the office, and would like to know the probability that it rains during the time he is on the way.\\n\\nInput:\\n\\nThe first line of input contains the number of test cases, T. Each of the following T lines contain two numbers, P and time. P denotes the probability that it will rain today and time is the time (in minutes), it will take for Raj to reach his office.\\n\\nOutput:\\n\\nOutput should have T lines each containing answer to corresponding test case. Please round the answer to 4 decimal places.\\n\\nConstraints:\\n\\n1 ≤ T ≤ 100\\n0 ≤ P ≤ 0.5\\n10 ≤ time ≤ 720\\ntime is a perfect divisor of 1440.\\n\\nSAMPLE INPUT\\n2\\n0 10\\n.5 720\\n\\nSAMPLE OUTPUT\\n0.0000\\n0.2929\",\n",
       " 'solutions': '[\"test_case = int(input())\\\\n\\\\nwhile test_case:\\\\n\\\\tin_1 = input()\\\\n\\\\tin_1 = in_1.split()\\\\n\\\\t\\\\n\\\\tp = float(in_1[0])\\\\n\\\\tt = int(in_1[1])\\\\n\\\\t\\\\n\\\\tprint((\\\\\"%.4f\\\\\" % (1-pow(1-p,t/1440.0))))\\\\n\\\\t\\\\t\\\\n\\\\ttest_case = test_case -1\\\\n\", \"T = int(input())\\\\nfor t in range(T):\\\\n\\\\tP,time = [float(x) for x in input().split()]\\\\n\\\\tprint(\\\\\"%.4f\\\\\" % (1-(1-P)**(time/1440.0)))\\\\n\", \"T = int(input())\\\\nfor t in range(T):\\\\n\\\\tP,time = [float(x) for x in input().split()]\\\\n\\\\tprint(\\\\\"%.4f\\\\\" % (1-(1-P)**(time/1440.0)))\\\\n\", \"T = int(input())\\\\nfor t in range(T):\\\\n\\\\tP,time = [float(x) for x in input().split()]\\\\n\\\\tprint(\\\\\"%.4f\\\\\" % (1-(1-P)**(time/1440.0)))\\\\n\"]',\n",
       " 'starter_code': '',\n",
       " 'input_output': '{\"inputs\": [\"10\\\\n0.04225316147702651 360\\\\n0.4705782523385532 360\\\\n0.08382464024053793 360\\\\n0.13255059454364293 360\\\\n0.21300375998356036 360\\\\n0.3502440976119032 360\\\\n0.010784341396946395 360\\\\n0.23707130989642777 360\\\\n0.49311228702508814 360\\\\n0.36037754459547156 360\\\\n\", \"10\\\\n0.39778418406102245 360\\\\n0.03678475905087364 360\\\\n0.45098594972954786 360\\\\n0.46414911004713866 360\\\\n0.1911047898260626 360\\\\n0.25150130998484777 360\\\\n0.28817524782217097 360\\\\n0.382550166174656 360\\\\n0.41416484028859224 360\\\\n0.027349350611655554 360\\\\n\", \"10\\\\n0.11467582006829746 360\\\\n0.28892244872065864 360\\\\n0.34042805781637586 360\\\\n0.14515462403418378 360\\\\n0.27869234034458656 360\\\\n0.19597557089011852 360\\\\n0.3788502368296178 360\\\\n0.022797909592525367 360\\\\n0.17810767977623654 360\\\\n0.3278154177017245 360\\\\n\", \"10\\\\n0.394859152259871 360\\\\n0.2906965103420224 360\\\\n0.10687523221434492 360\\\\n0.46674469298933063 360\\\\n0.35299008843376645 360\\\\n0.30878262927197664 360\\\\n0.21356966716028147 360\\\\n0.30560559964675515 360\\\\n0.3017355223471274 360\\\\n0.38784305951821174 360\\\\n\", \"10\\\\n0.23057322142972614 360\\\\n0.17250953948368541 360\\\\n0.4064599400511799 360\\\\n0.07847887873690329 360\\\\n0.49156112030734056 360\\\\n0.41208181269545874 360\\\\n0.07808425663860508 360\\\\n0.1211095435085412 60\\\\n0.2757797180143483 360\\\\n0.15138514664598268 144\\\\n\", \"10\\\\n0.07754807350042381 360\\\\n0.1699271350117204 360\\\\n0.11012448515172757 360\\\\n0.4373350776519923 360\\\\n0.08977046555393031 360\\\\n0.011426844730894614 360\\\\n0.037337077019781284 360\\\\n0.29309705805225483 360\\\\n0.15373180044523727 360\\\\n0.10294787733980815 360\\\\n\", \"10\\\\n0.23638159159934402 360\\\\n0.3462970467595584 360\\\\n0.2818152000382953 360\\\\n0.1908522406582429 360\\\\n0.13814366467827865 360\\\\n0.08328685634537347 360\\\\n0.0541062251578166 360\\\\n0.022438531122368044 360\\\\n0.16854517001674985 360\\\\n0.38983062036387695 360\\\\n\", \"10\\\\n0.4775511206846237 360\\\\n0.4264865525944467 360\\\\n0.3720138182672015 360\\\\n0.16748274070808689 360\\\\n0.29712741500154116 360\\\\n0.2316433407060845 360\\\\n0.39165662564444903 360\\\\n0.48625502694035894 360\\\\n0.30036188128623165 360\\\\n0.4391154862334754 24\\\\n\", \"10\\\\n0.18645496223766345 360\\\\n0.17989902622977394 360\\\\n0.45262248490623425 360\\\\n0.04663296744034651 360\\\\n0.12725823257381275 360\\\\n0.2895663014860297 48\\\\n0.05625277914818627 360\\\\n0.11455689444280615 360\\\\n0.46003084917418513 360\\\\n0.17986394802612649 360\\\\n\", \"10\\\\n0.26855286354198316 360\\\\n0.4845078440769134 360\\\\n0.41277817875080214 360\\\\n0.22279173466341384 360\\\\n0.04484753602630531 360\\\\n0.4832395613538977 360\\\\n0.345797814722544 360\\\\n0.14518834077354936 360\\\\n0.25582830206284146 360\\\\n0.4516072418699678 360\\\\n\"], \"outputs\": [\"0.0200\\\\n0.0455\\\\n0.0287\\\\n0.1339\\\\n0.0232\\\\n0.0029\\\\n0.0095\\\\n0.0831\\\\n0.0409\\\\n0.0268\\\\n\", \"0.0300\\\\n0.0817\\\\n0.0988\\\\n0.0384\\\\n0.0784\\\\n0.0531\\\\n0.1122\\\\n0.0057\\\\n0.0479\\\\n0.0945\\\\n\", \"0.0634\\\\n0.0462\\\\n0.1223\\\\n0.0202\\\\n0.1556\\\\n0.1244\\\\n0.0201\\\\n0.0054\\\\n0.0775\\\\n0.0163\\\\n\", \"0.1191\\\\n0.0093\\\\n0.1392\\\\n0.1444\\\\n0.0516\\\\n0.0699\\\\n0.0815\\\\n0.1136\\\\n0.1251\\\\n0.0069\\\\n\", \"0.0652\\\\n0.1008\\\\n0.0794\\\\n0.0516\\\\n0.0365\\\\n0.0215\\\\n0.0138\\\\n0.0057\\\\n0.0451\\\\n0.1162\\\\n\", \"0.1498\\\\n0.1298\\\\n0.1098\\\\n0.0448\\\\n0.0844\\\\n0.0638\\\\n0.1168\\\\n0.1534\\\\n0.0854\\\\n0.0096\\\\n\", \"0.0752\\\\n0.1527\\\\n0.1246\\\\n0.0611\\\\n0.0114\\\\n0.1521\\\\n0.1007\\\\n0.0385\\\\n0.0712\\\\n0.1395\\\\n\", \"0.0107\\\\n0.1470\\\\n0.0216\\\\n0.0349\\\\n0.0581\\\\n0.1022\\\\n0.0027\\\\n0.0654\\\\n0.1562\\\\n0.1057\\\\n\", \"0.0503\\\\n0.0484\\\\n0.1399\\\\n0.0119\\\\n0.0335\\\\n0.0113\\\\n0.0144\\\\n0.0300\\\\n0.1428\\\\n0.0484\\\\n\", \"0.1180\\\\n0.0823\\\\n0.0279\\\\n0.1455\\\\n0.1031\\\\n0.0882\\\\n0.0583\\\\n0.0871\\\\n0.0859\\\\n0.1155\\\\n\"]}',\n",
       " 'difficulty': 'MEDIUM',\n",
       " 'raw_tags': \"['Probability', 'Math']\",\n",
       " 'name': 'so-random',\n",
       " 'source': 'hackerearth',\n",
       " 'tags': \"['Mathematics', 'Probability']\",\n",
       " 'skill_types': '[]',\n",
       " 'url': None,\n",
       " 'Expected Auxiliary Space': None,\n",
       " 'time_limit': None,\n",
       " 'date': None,\n",
       " 'picture_num': None,\n",
       " 'memory_limit': None,\n",
       " 'Expected Time Complexity': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[2545]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2545])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_generation(input_file):\n",
    "    generations = {}\n",
    "    with open(input_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "        for _, res in enumerate(results):\n",
    "            task_id = res['task_id']\n",
    "            output = res['output']\n",
    "            generations[task_id] = output\n",
    "    return generations\n",
    "\n",
    "load_generation(f\"{config['parse_configs']['saving_path']}/no_context_parsed.json\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"It's the rainy season again, and the city experiences frequent showers throughout the day.\\n\\nThe weather report says that there is a P probability of rainfalls today. Raj has to step out for a meeting at the office, and would like to know the probability that it rains during the time he is on the way.\\n\\nInput:\\n\\nThe first line of input contains the number of test cases, T. Each of the following T lines contain two numbers, P and time. P denotes the probability that it will rain today and time is the time (in minutes), it will take for Raj to reach his office.\\n\\nOutput:\\n\\nOutput should have T lines each containing answer to corresponding test case. Please round the answer to 4 decimal places.\\n\\nConstraints:\\n\\n1 ≤ T ≤ 100\\n0 ≤ P ≤ 0.5\\n10 ≤ time ≤ 720\\ntime is a perfect divisor of 1440.\\n\\nSAMPLE INPUT\\n2\\n0 10\\n.5 720\\n\\nSAMPLE OUTPUT\\n0.0000\\n0.2929\",\n",
       " 'solutions': '[\"test_case = int(input())\\\\n\\\\nwhile test_case:\\\\n\\\\tin_1 = input()\\\\n\\\\tin_1 = in_1.split()\\\\n\\\\t\\\\n\\\\tp = float(in_1[0])\\\\n\\\\tt = int(in_1[1])\\\\n\\\\t\\\\n\\\\tprint((\\\\\"%.4f\\\\\" % (1-pow(1-p,t/1440.0))))\\\\n\\\\t\\\\t\\\\n\\\\ttest_case = test_case -1\\\\n\", \"T = int(input())\\\\nfor t in range(T):\\\\n\\\\tP,time = [float(x) for x in input().split()]\\\\n\\\\tprint(\\\\\"%.4f\\\\\" % (1-(1-P)**(time/1440.0)))\\\\n\", \"T = int(input())\\\\nfor t in range(T):\\\\n\\\\tP,time = [float(x) for x in input().split()]\\\\n\\\\tprint(\\\\\"%.4f\\\\\" % (1-(1-P)**(time/1440.0)))\\\\n\", \"T = int(input())\\\\nfor t in range(T):\\\\n\\\\tP,time = [float(x) for x in input().split()]\\\\n\\\\tprint(\\\\\"%.4f\\\\\" % (1-(1-P)**(time/1440.0)))\\\\n\"]',\n",
       " 'starter_code': '',\n",
       " 'input_output': '{\"inputs\": [\"10\\\\n0.04225316147702651 360\\\\n0.4705782523385532 360\\\\n0.08382464024053793 360\\\\n0.13255059454364293 360\\\\n0.21300375998356036 360\\\\n0.3502440976119032 360\\\\n0.010784341396946395 360\\\\n0.23707130989642777 360\\\\n0.49311228702508814 360\\\\n0.36037754459547156 360\\\\n\", \"10\\\\n0.39778418406102245 360\\\\n0.03678475905087364 360\\\\n0.45098594972954786 360\\\\n0.46414911004713866 360\\\\n0.1911047898260626 360\\\\n0.25150130998484777 360\\\\n0.28817524782217097 360\\\\n0.382550166174656 360\\\\n0.41416484028859224 360\\\\n0.027349350611655554 360\\\\n\", \"10\\\\n0.11467582006829746 360\\\\n0.28892244872065864 360\\\\n0.34042805781637586 360\\\\n0.14515462403418378 360\\\\n0.27869234034458656 360\\\\n0.19597557089011852 360\\\\n0.3788502368296178 360\\\\n0.022797909592525367 360\\\\n0.17810767977623654 360\\\\n0.3278154177017245 360\\\\n\", \"10\\\\n0.394859152259871 360\\\\n0.2906965103420224 360\\\\n0.10687523221434492 360\\\\n0.46674469298933063 360\\\\n0.35299008843376645 360\\\\n0.30878262927197664 360\\\\n0.21356966716028147 360\\\\n0.30560559964675515 360\\\\n0.3017355223471274 360\\\\n0.38784305951821174 360\\\\n\", \"10\\\\n0.23057322142972614 360\\\\n0.17250953948368541 360\\\\n0.4064599400511799 360\\\\n0.07847887873690329 360\\\\n0.49156112030734056 360\\\\n0.41208181269545874 360\\\\n0.07808425663860508 360\\\\n0.1211095435085412 60\\\\n0.2757797180143483 360\\\\n0.15138514664598268 144\\\\n\", \"10\\\\n0.07754807350042381 360\\\\n0.1699271350117204 360\\\\n0.11012448515172757 360\\\\n0.4373350776519923 360\\\\n0.08977046555393031 360\\\\n0.011426844730894614 360\\\\n0.037337077019781284 360\\\\n0.29309705805225483 360\\\\n0.15373180044523727 360\\\\n0.10294787733980815 360\\\\n\", \"10\\\\n0.23638159159934402 360\\\\n0.3462970467595584 360\\\\n0.2818152000382953 360\\\\n0.1908522406582429 360\\\\n0.13814366467827865 360\\\\n0.08328685634537347 360\\\\n0.0541062251578166 360\\\\n0.022438531122368044 360\\\\n0.16854517001674985 360\\\\n0.38983062036387695 360\\\\n\", \"10\\\\n0.4775511206846237 360\\\\n0.4264865525944467 360\\\\n0.3720138182672015 360\\\\n0.16748274070808689 360\\\\n0.29712741500154116 360\\\\n0.2316433407060845 360\\\\n0.39165662564444903 360\\\\n0.48625502694035894 360\\\\n0.30036188128623165 360\\\\n0.4391154862334754 24\\\\n\", \"10\\\\n0.18645496223766345 360\\\\n0.17989902622977394 360\\\\n0.45262248490623425 360\\\\n0.04663296744034651 360\\\\n0.12725823257381275 360\\\\n0.2895663014860297 48\\\\n0.05625277914818627 360\\\\n0.11455689444280615 360\\\\n0.46003084917418513 360\\\\n0.17986394802612649 360\\\\n\", \"10\\\\n0.26855286354198316 360\\\\n0.4845078440769134 360\\\\n0.41277817875080214 360\\\\n0.22279173466341384 360\\\\n0.04484753602630531 360\\\\n0.4832395613538977 360\\\\n0.345797814722544 360\\\\n0.14518834077354936 360\\\\n0.25582830206284146 360\\\\n0.4516072418699678 360\\\\n\"], \"outputs\": [\"0.0200\\\\n0.0455\\\\n0.0287\\\\n0.1339\\\\n0.0232\\\\n0.0029\\\\n0.0095\\\\n0.0831\\\\n0.0409\\\\n0.0268\\\\n\", \"0.0300\\\\n0.0817\\\\n0.0988\\\\n0.0384\\\\n0.0784\\\\n0.0531\\\\n0.1122\\\\n0.0057\\\\n0.0479\\\\n0.0945\\\\n\", \"0.0634\\\\n0.0462\\\\n0.1223\\\\n0.0202\\\\n0.1556\\\\n0.1244\\\\n0.0201\\\\n0.0054\\\\n0.0775\\\\n0.0163\\\\n\", \"0.1191\\\\n0.0093\\\\n0.1392\\\\n0.1444\\\\n0.0516\\\\n0.0699\\\\n0.0815\\\\n0.1136\\\\n0.1251\\\\n0.0069\\\\n\", \"0.0652\\\\n0.1008\\\\n0.0794\\\\n0.0516\\\\n0.0365\\\\n0.0215\\\\n0.0138\\\\n0.0057\\\\n0.0451\\\\n0.1162\\\\n\", \"0.1498\\\\n0.1298\\\\n0.1098\\\\n0.0448\\\\n0.0844\\\\n0.0638\\\\n0.1168\\\\n0.1534\\\\n0.0854\\\\n0.0096\\\\n\", \"0.0752\\\\n0.1527\\\\n0.1246\\\\n0.0611\\\\n0.0114\\\\n0.1521\\\\n0.1007\\\\n0.0385\\\\n0.0712\\\\n0.1395\\\\n\", \"0.0107\\\\n0.1470\\\\n0.0216\\\\n0.0349\\\\n0.0581\\\\n0.1022\\\\n0.0027\\\\n0.0654\\\\n0.1562\\\\n0.1057\\\\n\", \"0.0503\\\\n0.0484\\\\n0.1399\\\\n0.0119\\\\n0.0335\\\\n0.0113\\\\n0.0144\\\\n0.0300\\\\n0.1428\\\\n0.0484\\\\n\", \"0.1180\\\\n0.0823\\\\n0.0279\\\\n0.1455\\\\n0.1031\\\\n0.0882\\\\n0.0583\\\\n0.0871\\\\n0.0859\\\\n0.1155\\\\n\"]}',\n",
       " 'difficulty': 'MEDIUM',\n",
       " 'raw_tags': \"['Probability', 'Math']\",\n",
       " 'name': 'so-random',\n",
       " 'source': 'hackerearth',\n",
       " 'tags': \"['Mathematics', 'Probability']\",\n",
       " 'skill_types': '[]',\n",
       " 'url': None,\n",
       " 'Expected Auxiliary Space': None,\n",
       " 'time_limit': None,\n",
       " 'date': None,\n",
       " 'picture_num': None,\n",
       " 'memory_limit': None,\n",
       " 'Expected Time Complexity': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict[2545]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: unmatched '[' (2816954221.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mf\"{config[\"parse_configs\"][\"saving_path\"]}/no_context_parsed.json\"\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m f-string: unmatched '['\n"
     ]
    }
   ],
   "source": [
    "f\"{config[\"parse_configs\"][\"saving_path\"]}/no_context_parsed.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_generation(json.load(open(\"no_context.json\")), 2545 , \"no_context_parsed.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_1_pass_by_test(\"no_context_parsed.json\", [train_dict[2545]], file=\"no_context_1_pass.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2545': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open(\"no_context_1_pass.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
